{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e754a47a-300d-41cb-9abe-7ce17ee2d172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erthax/.pyenv/versions/3.11.4/envs/emotion-recognition/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from IPython.display import display, clear_output\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "420d6a9e-66cd-4fdb-9eea-3c5884878631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/erthax/.pyenv/versions/3.11.4/envs/emotion-recognition/lib/python3.11/site-packages/cv2/qt/plugins\"\n"
     ]
    }
   ],
   "source": [
    "# Simple test with a static image\n",
    "test_image = np.random.rand(160, 160, 3) * 255  # Random image\n",
    "test_image = test_image.astype(np.uint8)\n",
    "test_image_resized = cv2.resize(test_image, (224, 224), interpolation=cv2.INTER_LINEAR)\n",
    "test_image_bgr = cv2.cvtColor(test_image_resized, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "cv2.imshow('Test Image', test_image_bgr)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f75e647a-5560-4ff7-a1bc-fe3f0e7049bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped by user\n"
     ]
    }
   ],
   "source": [
    "vid = cv2.VideoCapture(0)\n",
    "vid.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "vid.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "# Create face detector\n",
    "mtcnn = MTCNN(select_largest=False)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = vid.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "\n",
    "        # Convert the image color to RGB\n",
    "        frame_draw = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        draw = ImageDraw.Draw(frame_draw)  # Create a drawing context\n",
    "\n",
    "        # Detect faces\n",
    "        boxes, probs, landmarks = mtcnn.detect(frame, landmarks=True)\n",
    "\n",
    "        if boxes is not None:\n",
    "            for box, landmark in zip(boxes, landmarks):\n",
    "                draw.rectangle(box.tolist(), outline=(255, 0, 0), width=6)\n",
    "                if landmark is not None:\n",
    "                    for point in landmark:\n",
    "                        x, y = point\n",
    "                        draw.ellipse((x-5, y-5, x+5, y+5), outline=(0, 255, 0), width=3)\n",
    "                x, y, w, h = box\n",
    "                text = \"Happy\"\n",
    "                # Optional: Use a pre-loaded font or a simpler default font\n",
    "                font = ImageFont.load_default()\n",
    "                draw.text((x, y-50), text, fill=(0, 0, 255), font=font)\n",
    "\n",
    "        # Convert PIL image back to array\n",
    "        frame_show = cv2.cvtColor(np.array(frame_draw), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Display image in the notebook\n",
    "        display(Image.fromarray(frame_show))\n",
    "        clear_output(wait=True)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopped by user\")\n",
    "finally:\n",
    "    vid.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7e0f95b-9564-40a7-b249-3e3313f71f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from facenet_pytorch import MTCNN\n",
    "from IPython.display import display\n",
    "\n",
    "# Assuming the model definition is correctly imported\n",
    "from POSTER_V2.models.PosterV2_7cls import *\n",
    "\n",
    "# Create video capture object\n",
    "vid = cv2.VideoCapture(0)\n",
    "vid.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "vid.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "# Create face detector\n",
    "mtcnn = MTCNN(select_largest=False)\n",
    "\n",
    "ret, frame = vid.read()\n",
    "vid.release()  # Release the video capture object immediately after capturing the frame\n",
    "\n",
    "\n",
    "\n",
    "if not ret:\n",
    "    print(\"Failed to grab frame\")\n",
    "else:\n",
    "    # Detect faces\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image_rgb = mtcnn(frame_rgb, save_path=\"./mue.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "653d11c9-b04e-4e30-af9e-f4d8757e28a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_transforms = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                    #transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225]),\n",
    "                    ])\n",
    "image_rgb = my_transforms(image_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f3ef69a-677d-4894-865e-17ab816731c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2138, -1.0579, -0.8629,  ..., -4.9325, -4.9178, -4.8983],\n",
       "         [-0.9409, -0.7627, -0.5232,  ..., -4.9074, -4.8900, -4.8593],\n",
       "         [-0.6485, -0.5204, -0.3254,  ..., -4.8385, -4.8273, -4.8106],\n",
       "         ...,\n",
       "         [ 2.2319,  2.2319,  2.2319,  ..., -6.0242, -6.0492, -6.0826],\n",
       "         [ 2.2319,  2.2319,  2.2319,  ..., -6.0430, -6.0618, -6.0924],\n",
       "         [ 2.2319,  2.2319,  2.2319,  ..., -6.0680, -6.0729, -6.0924]],\n",
       "\n",
       "        [[-2.3322, -2.2126, -2.0332,  ..., -5.3765, -5.3515, -5.3316],\n",
       "         [-1.9336, -1.7912, -1.5663,  ..., -5.3309, -5.3117, -5.2918],\n",
       "         [-1.5350, -1.4382, -1.2631,  ..., -5.2797, -5.2704, -5.2619],\n",
       "         ...,\n",
       "         [ 2.3414,  2.3414,  2.3414,  ..., -6.0640, -6.0790, -6.0989],\n",
       "         [ 2.3414,  2.3414,  2.3414,  ..., -6.0790, -6.0875, -6.0989],\n",
       "         [ 2.3414,  2.3414,  2.3414,  ..., -6.0989, -6.0989, -6.0989]],\n",
       "\n",
       "        [[-2.1690, -2.0698, -1.9061,  ..., -5.1303, -5.1502, -5.1899],\n",
       "         [-1.7722, -1.6617, -1.4611,  ..., -5.0907, -5.1190, -5.1700],\n",
       "         [-1.3754, -1.3130, -1.1614,  ..., -5.0340, -5.0531, -5.0956],\n",
       "         ...,\n",
       "         [ 2.5681,  2.5794,  2.5879,  ..., -5.7107, -5.7362, -5.7702],\n",
       "         [ 2.5879,  2.5879,  2.5879,  ..., -5.7001, -5.7383, -5.7950],\n",
       "         [ 2.5879,  2.5879,  2.5879,  ..., -5.6859, -5.7355, -5.8149]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081c4fb0-d0ba-43d1-ae6b-b68518f19b63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
